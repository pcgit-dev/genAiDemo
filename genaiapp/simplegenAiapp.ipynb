{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here are the key points related to setting limits on LangSmith production usage:\n",
      "\n",
      "1. **Setting Production Limits**: \n",
      "   - The total traces limit should be based on the expected load of traces sent to LangSmith. \n",
      "   - Example: If the gen AI application is called 1.2-1.5 times per second, resulting in 100,000-130,000 traces per day, and expecting to double in size, a suitable monthly limit would be 7,800,000 traces (i.e., 130,000 traces/day * 2 * 30 days).\n",
      "\n",
      "2. **Cost Management**:\n",
      "   - By setting appropriate limits, the maximum cost can be reduced significantly (e.g., from ~$40k to ~$7.5k per month) by avoiding expensive data retention upgrades.\n",
      "\n",
      "3. **Extended Data Retention**:\n",
      "   - Extended data retention can affect other features if the limit is reached. It’s important to understand its functionality before using it.\n",
      "\n",
      "4. **Dev/Staging Environment Limits**:\n",
      "   - Dev and staging environments should have limits set at 10% of the production limit, though this can vary based on specific use cases, especially if running evaluations as part of CI/CD processes.\n",
      "\n",
      "5. **Usage Tracking and Management**:\n",
      "   - LangSmith tracks traces per workspace, often representing different environments or teams. Understanding spend at this granular level helps in managing costs effectively.\n",
      "\n",
      "6. **Data Retention Strategy**:\n",
      "   - To retain historical data for debugging, server-side sampling for extended data retention is recommended. A suggested starting point is sampling 10% of runs, but the exact percentage should be determined based on specific needs.\n",
      "\n",
      "7. **Automations and Run Rules**:\n",
      "   - LangSmith’s automations product allows for setting run rules to automatically upgrade data retention for specific traces, which can be configured on the projects page.\n"
     ]
    }
   ],
   "source": [
    "##Simple gen AI app using langchain\n",
    "##Load Data -->Docs-->Devide text to cunks -->text -->vectors embedding-->vectors-->vector stor db-->\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "#Data Ingetion from website\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/administration/tutorials/manage_spend\")\n",
    "documents = loader.load()\n",
    "\n",
    "textSplitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=100)\n",
    "doctsInChunks = textSplitter.split_documents(documents)\n",
    "\n",
    "open_api_key = os.getenv(key=\"OPEN_API_KEY2\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\",api_key=open_api_key,dimensions=1024)\n",
    "\n",
    "#Saving and querying from vector DB\n",
    "vectorStoreDb = FAISS.from_documents(doctsInChunks,embeddings)\n",
    "query = \"LangSmith has two usage limits: total traces and extended retention traces\"\n",
    "result = vectorStoreDb.similarity_search(query)\n",
    "\n",
    "#Retrieval/Document Chain\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\" \n",
    "     Answer the following context based only on the provided context:\n",
    "     <context>\n",
    "     {context}\n",
    "     </context> \n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\",api_key=open_api_key)\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "document_chain.invoke({\n",
    "    \"input\":\"LangSmith has two usage limits: total traces and extended\",\n",
    "       \"context\":[Document(page_content=\"LangSmith has two usage limits: total traces and extended traces. These correspond to the two metrics we ve been tracking on our usage graph.\")]\n",
    "    })\n",
    "\n",
    "##Retriever -> Way of getting the data from vector DB.\n",
    "retriever = vectorStoreDb.as_retriever()\n",
    "\n",
    "retrival_chain=create_retrieval_chain(retriever,document_chain)\n",
    "\n",
    "#Get the response form LLM\n",
    "response = retrival_chain.invoke({\"input\":\"LangSmith has two usage limits: total traces and extended retention traces\"})\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
